\chapter{Parallel simulation}
\label{cha:parallel_sim}
A sequential simulation is executed on a single processor, either logical, physical processor or machine within a cluster.
Simulating complex models and a huge number of events can result in a long execution time with increasing complexity this leads to the demand for a improved execution method.
The distributed discrete event simulation (\emph{DDES}) or parallel discrete event simulation (\emph{PDES}) allows the parallel execution on multiple processors.
The goal of parallel execution is the improvement of simulation performance and reduction of required execution time.
Within a parallel simulation events are processed by different logical processes (\emph{LP}).
Each \emph{LP} can be executed by a different separate processing unit and therefore executed in parallel. \cite{bagrodia_parsec_1998}

This distributed processing of events require synchronization for guaranteeing the causality constraint, i.e. the processing of all events within a \emph{LP} is happening with increasing timestamps.

\section{Synchronization}
\label{sec:parallel_synchronization}
Running simulations in parallel requires a synchronization in between the \emph{LPs} for synchronizing the processed simulation time.
This simulation time can diverge very strongly due to the different set of processed events.
With varying event intervals (i.e. time in between two consecutive events) the steps within simulation time are differentiating.
For the synchronization of parallel \emph{LPs} different algorithms are available, the choice of the used synchronization method affects the achievable performance of the \emph{PDES}. \cite[chapter 2]{bagrodia_performance_2000}

Generally there are two types of synchronization algorithms:

\begin{enumerate}
    \item Optimistic algorithms allows \emph{LPs} to process events without guaranteeing that no new events may be scheduled in a past point in simulation time.
    In the case of an event which will be schedules in the past a \emph{roll back} mechanism is necessary.
    \item Conservative algorithms strictly forbid the processing of events until the guarantee can be given that no new event will be scheduled before this events.
\end{enumerate}

An example for a conservative algorithm is the \emph{Null Message Algorithm} (\emph{NMA}). \cite[section 2.1]{bagrodia_performance_2000} \cite{Varga03apractical}.
The synchronization method is exchanging information from one \emph{LP} to another.
A communication method is required which allows this exchange of information.

\section{Communication}
\label{sec:parallel_communication}
Depending on the used hardware and the type of distribution of the processing unit different communication methods can be used and allow an optimal performance.
For this communication every inter-thread, inter-process or inter-machine communication method can be used.

The synchronization requires the transmission of messages therefore the demanded communication represents a message passing interface \emph{MPI}.
On different operating systems and depending on the specific simulation setup the \emph{MPI} can be based on various transport technologies.
For the simulations running on a linux operating system the open source library openMPI provides a implementation for this usage. \cite{openmpi_hp}

\section{Parallel simulation with OMNeT++}
\label{sec:parallel_omnet}

OMNeT++ provides the functionality of running a simulation in parallel with no need for modifications of the simulated model, in terms of source code.
This is achieved by the configuration files which include all \emph{PDES} relevant parameter and settings.
Therefore the decision for a parallel simulation can be made during runtime and realized via different configurations. \cite[section III.A]{varga_parallel_2003}

For a parallel simulation using OMNeT++ different implementations for communication and synchronization must be defined and specific requirements must be met.
These factors are discussed in the following sections.

\subsection{Communication}
\label{sec:parallel_omnet_comm}

OMNeT++ provides a defined interface for parallel simulation communication represented by the class \emph{cParsimCommunications}.
By subclassing \emph{cParsimCommunications} any implemented communication method can be used for parallel simulation.
These built in implementations are available in OMNeT++: \cite{omnet_par_api}

\begin{description}
    \item[cFileCommunications] represents the communication via textfiles.
                               Due to the necessary file operations and the big dependency of the disk where this file is located this communication method is more recommended for debugging purposes.
    \item[cNamedPipeCommunications] is using named pipes for the transport of messages.
    \item[cMPICommunications] 
\end{description}


\subsection{Sychronization}
\label{sec:parallel_omnet_sync}

\subsection{Requriements}
\label{sec:parallel_omnet_requirements}

% imported from paper
As described in \cite{varga_parallel_2003} OMNeT++ is capable of running a parallel simulation when specific requirements are met.
In \cite{varga_parallel_2003} the requirements include the compliance to OMNeT++ design guides, for example the strict usage of messages and channels for transmitting events and data.
Is this requirement not met and communication between modules was achieved by a direct method call the simulation cannot be executed parallel.
Running the simulation parallelized can achieve better timings and improve the performance of real time simulations
This improvement could be possible due to shortened execution times and also the extended possibility for emulation and the field of \emph{HiL}.
For this usage a module can be assigned to a logical processor and handle all messages which interfere with connected real systems.